
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import io

from lib.text_utils import is_noise_token

st.set_page_config(page_title="â‘  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º", page_icon="ğŸ”‘", layout="wide")
st.title("ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆåç›Šå¯„ä¸ã®å¯è¦–åŒ–ï¼‰")

if "df" not in st.session_state:
    st.warning("ã¾ãšãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã§Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

df = st.session_state["df"].copy()
meta = st.session_state.get("meta", {})

# åˆ—é¸æŠ
text_col = st.selectbox("å°æœ¬ãƒ†ã‚­ã‚¹ãƒˆåˆ—", options=list(df.columns), index=list(df.columns).index(meta.get("text_col")) if meta.get("text_col") in df.columns else 0)
target_col = st.selectbox("åç›Šï¼ˆç›®çš„å¤‰æ•°ï¼‰åˆ—", options=[c for c in df.columns if c != text_col], index=list(df.columns).index(meta.get("profit_col")) if meta.get("profit_col") in df.columns else 0)

# ãƒã‚¤ãƒ‘ãƒ©
with st.sidebar:
    st.header("æŠ½å‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    ngram_min = st.slider("n-gramæœ€å°", 2, 4, 2)
    ngram_max = st.slider("n-gramæœ€å¤§", ngram_min, 6, 4)
    min_df = st.slider("min_dfï¼ˆæœ€ä½å‡ºç¾æ•°ï¼‰", 3, 50, 10)
    max_features = st.slider("max_features", 1000, 20000, 5000, step=500)
    svd_components = st.slider("SVDæˆåˆ†æ•°ï¼ˆåœ§ç¸®æ¬¡å…ƒï¼‰", 20, 300, 80, step=10)
    top_k = st.slider("ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 5, 100, 30, step=5)
    sample_n = st.slider("ã‚µãƒ³ãƒ—ãƒ«ä»¶æ•°ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰", 500, len(df), min(1500, len(df)), step=100)

# å‰å‡¦ç†
df["_text"] = df[text_col].fillna("").astype(str)
y_raw = pd.to_numeric(df[target_col], errors="coerce")
use = df[y_raw.notna()].copy()
use["y"] = pd.to_numeric(use[target_col], errors="coerce")
if len(use) < 50:
    st.error("ç›®çš„å¤‰æ•°ï¼ˆåç›Šï¼‰ã«æœ‰åŠ¹ãªæ•°å€¤ãŒ50ä»¶æœªæº€ã§ã™ã€‚åˆ—é¸æŠã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

if len(use) > sample_n:
    use = use.sample(sample_n, random_state=42)

# TF-IDFï¼ˆchar n-gramï¼‰
vectorizer = TfidfVectorizer(
    analyzer="char",
    ngram_range=(ngram_min, ngram_max),
    min_df=min_df,
    max_features=max_features
)
X_tfidf = vectorizer.fit_transform(use["_text"].values)
vocab = np.array(vectorizer.get_feature_names_out())

# ãƒã‚¤ã‚ºn-gramã®ãƒã‚¹ã‚¯ã‚’ä½œæˆï¼ˆå¾Œã§é‡è¦åº¦é›†è¨ˆæ™‚ã«é™¤å¤–ï¼‰
noise_mask = np.array([is_noise_token(tok) for tok in vocab])

# æ¬¡å…ƒåœ§ç¸®ï¼‹å›å¸°
svd = TruncatedSVD(n_components=svd_components, random_state=42)
X_svd = svd.fit_transform(X_tfidf)

X_train, X_test, y_train, y_test = train_test_split(X_svd, use["y"].values.astype(float), test_size=0.2, random_state=42)
model = Ridge(alpha=1.0, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

st.subheader("ãƒ¢ãƒ‡ãƒ«æŒ‡æ¨™")
st.write({"R2": round(float(r2), 4), "MAE": round(float(mae), 2), "n_train": int(len(X_train)), "n_test": int(len(X_test))})

# ä¿‚æ•°ã‚’åŸèªå½™ç©ºé–“ã¸è¿‘ä¼¼é€†å†™åƒã—ã¦å„n-gramã®é‡è¦åº¦ã‚’æ¨å®š
coef_svd = model.coef_
V = svd.components_  # (n_components, n_features)
approx_importance = V.T @ coef_svd  # (n_features,)

# ãƒã‚¤ã‚ºé™¤å¤–
approx_importance = np.where(noise_mask, 0.0, approx_importance)

order_pos = np.argsort(approx_importance)[::-1]
order_neg = np.argsort(approx_importance)

top_pos = pd.DataFrame({"ngram": vocab[order_pos[:top_k]], "importance": approx_importance[order_pos[:top_k]]})
top_neg = pd.DataFrame({"ngram": vocab[order_neg[:top_k]], "importance": approx_importance[order_neg[:top_k]]})

col1, col2 = st.columns(2)
with col1:
    st.markdown("#### åç›Šã« **æ­£** ã®å½±éŸ¿ãŒå¤§ãã„ n-gram")
    fig1 = plt.figure(figsize=(7, 6))
    plt.barh(top_pos["ngram"][::-1], top_pos["importance"][::-1])
    plt.title("æ­£ã®å¯„ä¸ï¼ˆä¸Šä½ï¼‰")
    plt.tight_layout()
    st.pyplot(fig1)
    st.dataframe(top_pos)

with col2:
    st.markdown("#### åç›Šã« **è² ** ã®å½±éŸ¿ãŒå¤§ãã„ n-gram")
    fig2 = plt.figure(figsize=(7, 6))
    plt.barh(top_neg["ngram"][::-1], top_neg["importance"][::-1])
    plt.title("è² ã®å¯„ä¸ï¼ˆä¸Šä½ï¼‰")
    plt.tight_layout()
    st.pyplot(fig2)
    st.dataframe(top_neg)

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨
out = pd.DataFrame({
    "ngram": vocab,
    "importance": approx_importance,
    "is_noise": noise_mask
}).sort_values("importance", ascending=False)

st.download_button(
    label="å…¨n-gramé‡è¦åº¦ã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=out.to_csv(index=False).encode("utf-8-sig"),
    file_name="ngram_importance.csv",
    mime="text/csv",
)

st.caption("æ³¨ï¼šæ—¥æœ¬èªã¯å½¢æ…‹ç´ è§£æãªã—ã§ã‚‚char n-gramã§å®‰å®šã—ã¦å‚¾å‘ãŒå–ã‚Œã¾ã™ã€‚è‹±æ•°å­—ã®ã¿ã®ç¾…åˆ—ã‚„URLã¯è‡ªå‹•ã§é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚")
