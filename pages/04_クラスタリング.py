import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import umap
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import japanize_matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

from lib.text_utils import simple_japanese_tokenize

st.set_page_config(page_title="â‘£ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°", page_icon="ðŸŽ¯", layout="wide")
st.title("ðŸŽ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†æž")

if "df" not in st.session_state:
    st.warning("ã¾ãšãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã§Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

df = st.session_state["df"].copy()
meta = st.session_state.get("meta", {})

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    st.subheader("1. ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    
    # DataFrameãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    with st.expander("ðŸ“Š DataFrameãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10è¡Œï¼‰", expanded=False):
        st.dataframe(df.head(10), use_container_width=True)
    
    # æ¬ æå€¤ãƒ¬ãƒãƒ¼ãƒˆ
    with st.expander("â“ æ¬ æå€¤ãƒ¬ãƒãƒ¼ãƒˆ", expanded=False):
        missing_report = []
        for col in df.columns:
            missing_count = df[col].isnull().sum()
            missing_pct = (missing_count / len(df)) * 100
            missing_report.append({
                "åˆ—å": col,
                "æ¬ ææ•°": missing_count,
                "æ¬ æçŽ‡(%)": f"{missing_pct:.1f}%"
            })
        st.dataframe(pd.DataFrame(missing_report), hide_index=True)
    
    # 2. ç‰¹å¾´é‡è¨­å®š
    st.subheader("2. ç‰¹å¾´é‡è¨­å®š")
    
    # æ•°å€¤åˆ—ã®é¸æŠž
    numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
    selected_numeric_cols = st.multiselect(
        "æ•°å€¤åˆ—ã‚’é¸æŠž", 
        options=numeric_cols,
        default=numeric_cols[:3] if len(numeric_cols) >= 3 else numeric_cols
    )
    
    # æ¨™æº–åŒ–ã‚¹ã‚¤ãƒƒãƒ
    standardize_numeric = st.checkbox("æ•°å€¤åˆ—ã‚’æ¨™æº–åŒ–", value=True)
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠž
    text_cols = [col for col in df.columns if not pd.api.types.is_numeric_dtype(df[col])]
    text_col = st.selectbox(
        "ãƒ†ã‚­ã‚¹ãƒˆåˆ—ï¼ˆ1ã¤ï¼‰", 
        options=["ãªã—"] + text_cols,
        index=1 if len(text_cols) > 0 else 0
    )
    
    # TF-IDFè¨­å®š
    if text_col != "ãªã—":
        use_tfidf = st.checkbox("TF-IDFåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨", value=True)
        
        if use_tfidf:
            vector_dim = st.slider("ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ", 100, 2000, 300, step=50)
            min_df = st.number_input("min_dfï¼ˆæœ€å°å‡ºç¾é »åº¦ï¼‰", min_value=1, max_value=100, value=5)
            max_df = st.number_input("max_dfï¼ˆæœ€å¤§å‡ºç¾é »åº¦ï¼‰", min_value=0.1, max_value=1.0, value=0.8, step=0.1)
            
            # æ—¥æœ¬èªžã¯æŒ‡å®šãªã—ã€è‹±èªžã¯ "english"
            use_stopwords = st.selectbox("ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰", ["ãªã—", "english"])
            stopwords = "english" if use_stopwords == "english" else None
            
            # è‡ªå‹•æ´¾ç”Ÿç‰¹å¾´
            st.write("**è‡ªå‹•æ´¾ç”Ÿç‰¹å¾´ï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ—å¯¾è±¡ï¼‰**")
            derive_char_count = st.checkbox("æ–‡å­—æ•°", value=True)
            derive_word_count = st.checkbox("å˜èªžæ•°", value=True) 
            derive_avg_word_len = st.checkbox("å¹³å‡æ–‡é•·", value=True)
    else:
        use_tfidf = False
    
    # 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    st.subheader("3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°è¨­å®š
    cluster_method = st.radio("ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®šæ–¹æ³•", ["ç›´æŽ¥æŒ‡å®š", "è‡ªå‹•æŽ¨å®šï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆæœ€å¤§ï¼‰"])
    
    if cluster_method == "ç›´æŽ¥æŒ‡å®š":
        n_clusters = st.slider("ã‚¯ãƒ©ã‚¹ã‚¿æ•° k", 2, 15, 5)
    else:
        k_range_start = st.slider("æŽ¢ç´¢ç¯„å›²ï¼ˆæœ€å°ï¼‰", 2, 10, 2)
        k_range_end = st.slider("æŽ¢ç´¢ç¯„å›²ï¼ˆæœ€å¤§ï¼‰", k_range_start + 1, 15, 10)
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    use_scaling = st.checkbox("StandardScaleré©ç”¨", value=True)
    
    # 4. æ¬¡å…ƒå‰Šæ¸›è¨­å®š
    st.subheader("4. æ¬¡å…ƒå‰Šæ¸›")
    
    reduction_method = st.selectbox("æ‰‹æ³•", ["t-SNE", "UMAP"])
    
    if reduction_method == "t-SNE":
        perplexity = st.slider("perplexity", 5, 50, 30)
        learning_rate = st.number_input("learning_rate", 10.0, 1000.0, 200.0, step=50.0)
        n_iter = st.slider("n_iter", 250, 2000, 1000, step=250)
    else:  # UMAP
        n_neighbors = st.slider("n_neighbors", 5, 50, 15)
        min_dist = st.number_input("min_dist", 0.0, 1.0, 0.1, step=0.05)
    
    # 5. å®Ÿè¡Œãƒœã‚¿ãƒ³
    st.subheader("5. å®Ÿè¡Œ")
    execute_btn = st.button("ðŸŽ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ", type="primary", use_container_width=True)

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
if execute_btn:
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ç‰¹å¾´é‡ä½œæˆ
    st.header("ðŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæžœ")
    
    with st.status("ç‰¹å¾´é‡ã‚’æº–å‚™ä¸­...", expanded=True) as status:
        st.write("æ•°å€¤ç‰¹å¾´é‡ã‚’å‡¦ç†ä¸­...")
        
        # æ•°å€¤ç‰¹å¾´é‡ã®æº–å‚™
        features_list = []
        feature_names = []
        
        if selected_numeric_cols:
            numeric_data = df[selected_numeric_cols].fillna(0)
            
            if standardize_numeric:
                scaler = StandardScaler()
                numeric_data = pd.DataFrame(
                    scaler.fit_transform(numeric_data),
                    columns=selected_numeric_cols,
                    index=df.index
                )
            
            features_list.append(numeric_data)
            feature_names.extend(selected_numeric_cols)
        
        # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã®æº–å‚™
        if text_col != "ãªã—" and use_tfidf:
            st.write("ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆTF-IDFï¼‰ã‚’å‡¦ç†ä¸­...")
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            text_data = df[text_col].fillna("").astype(str)
            
            # TF-IDF
            def japanese_tokenizer(text):
                return simple_japanese_tokenize(text)
            
            vectorizer = TfidfVectorizer(
                tokenizer=japanese_tokenizer,
                max_features=vector_dim,
                min_df=min_df,
                max_df=max_df,
                stop_words=stopwords
            )
            
            try:
                tfidf_matrix = vectorizer.fit_transform(text_data)
                tfidf_df = pd.DataFrame(
                    tfidf_matrix.toarray(),
                    columns=[f"tfidf_{i}" for i in range(tfidf_matrix.shape[1])],
                    index=df.index
                )
                features_list.append(tfidf_df)
                feature_names.extend(tfidf_df.columns.tolist())
                
                # èªžå½™ã‚’ä¿å­˜ï¼ˆå¾Œã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰
                vocab = vectorizer.get_feature_names_out()
                
            except Exception as e:
                st.error(f"TF-IDFå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.stop()
            
            # è‡ªå‹•æ´¾ç”Ÿç‰¹å¾´
            if derive_char_count or derive_word_count or derive_avg_word_len:
                st.write("æ´¾ç”Ÿç‰¹å¾´é‡ã‚’å‡¦ç†ä¸­...")
                derived_features = pd.DataFrame(index=df.index)
                
                if derive_char_count:
                    derived_features["char_count"] = text_data.str.len()
                    feature_names.append("char_count")
                
                if derive_word_count:
                    derived_features["word_count"] = text_data.apply(lambda x: len(simple_japanese_tokenize(x)))
                    feature_names.append("word_count")
                
                if derive_avg_word_len:
                    derived_features["avg_word_len"] = text_data.apply(
                        lambda x: np.mean([len(word) for word in simple_japanese_tokenize(x)]) if simple_japanese_tokenize(x) else 0
                    )
                    feature_names.append("avg_word_len")
                
                if standardize_numeric:
                    scaler_derived = StandardScaler()
                    derived_features = pd.DataFrame(
                        scaler_derived.fit_transform(derived_features),
                        columns=derived_features.columns,
                        index=df.index
                    )
                
                features_list.append(derived_features)
        
        # å…¨ç‰¹å¾´é‡ã‚’çµåˆ
        if features_list:
            X = pd.concat(features_list, axis=1)
            st.write(f"ç‰¹å¾´é‡ä½œæˆå®Œäº†: {X.shape[1]}æ¬¡å…ƒ Ã— {X.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")
        else:
            st.error("ç‰¹å¾´é‡ãŒé¸æŠžã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.stop()
        
        status.update(label="ç‰¹å¾´é‡æº–å‚™å®Œäº†!", state="complete")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š
    with st.status("æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æŽ¢ç´¢ä¸­...", expanded=True) as status:
        if cluster_method == "è‡ªå‹•æŽ¨å®šï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆæœ€å¤§ï¼‰":
            st.write("ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æžã‚’å®Ÿè¡Œä¸­...")
            
            # æŽ¢ç´¢ç¯„å›²ã§ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            silhouette_scores = []
            k_range = range(k_range_start, k_range_end + 1)
            
            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42)
                cluster_labels = kmeans.fit_predict(X)
                sil_score = silhouette_score(X, cluster_labels)
                silhouette_scores.append(sil_score)
                st.write(f"k={k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {sil_score:.3f}")
            
            # æœ€é©ãªkã‚’é¸æŠž
            best_idx = np.argmax(silhouette_scores)
            n_clusters = k_range[best_idx]
            best_silhouette = silhouette_scores[best_idx]
            
            st.write(f"**æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters} (ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {best_silhouette:.3f})**")
        
        status.update(label="ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®šå®Œäº†!", state="complete")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    with st.status("K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...", expanded=True) as status:
        # æœ€çµ‚çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if use_scaling:
            scaler_final = StandardScaler()
            X_scaled = scaler_final.fit_transform(X)
        else:
            X_scaled = X.values
        
        # K-meanså®Ÿè¡Œ
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
        final_silhouette = silhouette_score(X_scaled, cluster_labels)
        st.write(f"æœ€çµ‚ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {final_silhouette:.3f}")
        
        status.update(label="ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†!", state="complete")
    
    # æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œ
    with st.status("æ¬¡å…ƒå‰Šæ¸›å®Ÿè¡Œä¸­...", expanded=True) as status:
        if reduction_method == "t-SNE":
            st.write("t-SNEå®Ÿè¡Œä¸­...")
            reducer = TSNE(
                n_components=2,
                perplexity=perplexity,
                learning_rate=learning_rate,
                n_iter=n_iter,
                random_state=42
            )
        else:
            st.write("UMAPå®Ÿè¡Œä¸­...")
            reducer = umap.UMAP(
                n_components=2,
                n_neighbors=n_neighbors,
                min_dist=min_dist,
                random_state=42
            )
        
        X_reduced = reducer.fit_transform(X_scaled)
        
        status.update(label="æ¬¡å…ƒå‰Šæ¸›å®Œäº†!", state="complete")
    
    # çµæžœã®å¯è¦–åŒ–
    st.subheader("ðŸ“ˆ ã‚¯ãƒ©ã‚¹ã‚¿è©•ä¾¡")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ã‚¨ãƒ«ãƒœãƒ¼æ³•
        st.write("**ã‚¨ãƒ«ãƒœãƒ¼æ³•ï¼ˆSSE vs kï¼‰**")
        k_range_elbow = range(2, 16)
        sse_scores = []
        
        for k in k_range_elbow:
            kmeans_temp = KMeans(n_clusters=k, random_state=42)
            kmeans_temp.fit(X_scaled)
            sse_scores.append(kmeans_temp.inertia_)
        
        fig_elbow = go.Figure()
        fig_elbow.add_trace(go.Scatter(
            x=list(k_range_elbow),
            y=sse_scores,
            mode='lines+markers',
            name='SSE'
        ))
        fig_elbow.update_layout(
            title="ã‚¨ãƒ«ãƒœãƒ¼æ³•",
            xaxis_title="ã‚¯ãƒ©ã‚¹ã‚¿æ•° k",
            yaxis_title="SSE (Sum of Squared Errors)",
            height=300
        )
        st.plotly_chart(fig_elbow, use_container_width=True)
    
    with col2:
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢
        st.write("**ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ï¼ˆk=2-15ï¼‰**")
        k_range_sil = range(2, 16)
        sil_scores_all = []
        
        for k in k_range_sil:
            kmeans_temp = KMeans(n_clusters=k, random_state=42)
            labels_temp = kmeans_temp.fit_predict(X_scaled)
            sil_score = silhouette_score(X_scaled, labels_temp)
            sil_scores_all.append(sil_score)
        
        colors = ['red' if k == n_clusters else 'blue' for k in k_range_sil]
        
        fig_sil = go.Figure()
        fig_sil.add_trace(go.Bar(
            x=list(k_range_sil),
            y=sil_scores_all,
            marker_color=colors,
            name='ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢'
        ))
        fig_sil.update_layout(
            title="ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ",
            xaxis_title="ã‚¯ãƒ©ã‚¹ã‚¿æ•° k", 
            yaxis_title="ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢",
            height=300
        )
        st.plotly_chart(fig_sil, use_container_width=True)
    
    # æœ€çµ‚ã‚¹ã‚³ã‚¢è¡¨ç¤º
    st.metric("é¸æŠžã•ã‚ŒãŸk", n_clusters, f"ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {final_silhouette:.3f}")
    
    # 2æ¬¡å…ƒæ•£å¸ƒå›³
    st.subheader("ðŸŽ¨ 2æ¬¡å…ƒå¯è¦–åŒ–")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆï¼ˆæ•£å¸ƒå›³ç”¨ï¼‰
    viz_df = pd.DataFrame({
        'x': X_reduced[:, 0],
        'y': X_reduced[:, 1], 
        'cluster': [f"Cluster {i}" for i in cluster_labels],
        'index': df.index
    })
    
    # è¿½åŠ æƒ…å ±ã‚’ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ç”¨ã«æº–å‚™
    tooltip_cols = ['index']
    
    # ã‚¿ã‚¤ãƒˆãƒ«åˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    title_candidates = ['title', 'ã‚¿ã‚¤ãƒˆãƒ«', 'å°æœ¬ãƒ‡ãƒ¼ã‚¿', 'å°æœ¬', 'æœ¬æ–‡']
    title_col = None
    for col in title_candidates:
        if col in df.columns:
            title_col = col
            break
    
    if title_col:
        viz_df['title'] = df[title_col].fillna("").astype(str).str[:50]  # 50æ–‡å­—ã¾ã§è¡¨ç¤º
        tooltip_cols.append('title')
    
    # revenueåˆ—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    revenue_candidates = ['revenue', 'profit', 'åŽç›Š', 'å£²ä¸Š', 'åˆ©ç›Š', 'åˆç®—ç²—åˆ©']
    revenue_col = None
    for col in revenue_candidates:
        if col in df.columns:
            revenue_col = col
            break
    
    if revenue_col:
        viz_df['revenue'] = df[revenue_col].fillna(0)
        tooltip_cols.append('revenue')
    
    # ä¸»è¦ç‰¹å¾´ã®ä¸€éƒ¨ã‚’è¿½åŠ 
    if selected_numeric_cols:
        for col in selected_numeric_cols[:3]:  # ä¸Šä½3ã¤ã¾ã§
            viz_df[f'{col}_feature'] = df[col].fillna(0)
            tooltip_cols.append(f'{col}_feature')
    
    viz_df['cluster_id'] = cluster_labels
    tooltip_cols.append('cluster_id')
    
    # Plotlyæ•£å¸ƒå›³ä½œæˆ
    fig_scatter = px.scatter(
        viz_df,
        x='x',
        y='y',
        color='cluster',
        hover_data=tooltip_cols,
        title=f"{reduction_method} 2æ¬¡å…ƒå¯è¦–åŒ– (k={n_clusters})",
        labels={'x': f'{reduction_method}_1', 'y': f'{reduction_method}_2'}
    )
    
    fig_scatter.update_traces(marker=dict(size=8, opacity=0.7))
    fig_scatter.update_layout(height=600)
    
    st.plotly_chart(fig_scatter, use_container_width=True)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    st.subheader("ðŸ“‹ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥çµ±è¨ˆ
    cluster_stats = []
    for i in range(n_clusters):
        cluster_mask = cluster_labels == i
        cluster_data = df[cluster_mask]
        
        stats = {
            'ã‚¯ãƒ©ã‚¹ã‚¿': f'Cluster {i}',
            'ä»¶æ•°': cluster_mask.sum()
        }
        
        # åŽç›Šçµ±è¨ˆ
        if revenue_col:
            revenue_data = pd.to_numeric(cluster_data[revenue_col], errors='coerce')
            if not revenue_data.dropna().empty:
                stats['åŽç›Š_å¹³å‡'] = f"{revenue_data.mean():.2f}"
                stats['åŽç›Š_ä¸­å¤®å€¤'] = f"{revenue_data.median():.2f}"
                stats['åŽç›Š_25%'] = f"{revenue_data.quantile(0.25):.2f}"
                stats['åŽç›Š_75%'] = f"{revenue_data.quantile(0.75):.2f}"
        
        cluster_stats.append(stats)
    
    cluster_df = pd.DataFrame(cluster_stats)
    st.dataframe(cluster_df, hide_index=True, use_container_width=True)
    
    # æ•°å€¤ç‰¹å¾´ã®å¹³å‡æ¯”è¼ƒ
    if selected_numeric_cols:
        st.write("**æ•°å€¤ç‰¹å¾´ã®å¹³å‡å€¤æ¯”è¼ƒ**")
        
        feature_means = []
        for i in range(n_clusters):
            cluster_mask = cluster_labels == i
            cluster_means = df[cluster_mask][selected_numeric_cols].mean()
            cluster_means['cluster'] = f'Cluster {i}'
            feature_means.append(cluster_means)
        
        means_df = pd.DataFrame(feature_means).set_index('cluster')
        
        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
        fig_radar = go.Figure()
        
        for cluster in means_df.index:
            fig_radar.add_trace(go.Scatterpolar(
                r=means_df.loc[cluster].values,
                theta=means_df.columns,
                fill='toself',
                name=cluster
            ))
        
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[means_df.values.min(), means_df.values.max()]
                )
            ),
            showlegend=True,
            title="ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥æ•°å€¤ç‰¹å¾´ã®å¹³å‡å€¤ãƒ¬ãƒ¼ãƒ€ãƒ¼"
        )
        
        st.plotly_chart(fig_radar, use_container_width=True)
    
    # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã®ãƒˆãƒƒãƒ—èªžå½™ï¼ˆTF-IDFä½¿ç”¨æ™‚ï¼‰
    if text_col != "ãªã—" and use_tfidf and 'vocab' in locals():
        st.write("**ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ãƒˆãƒƒãƒ—èªžå½™ï¼ˆä¸Šä½20èªžï¼‰**")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®TF-IDFç‰¹å¾´é‡ã‚’é›†è¨ˆ
        for i in range(n_clusters):
            cluster_mask = cluster_labels == i
            cluster_tfidf = tfidf_df[cluster_mask]
            
            if len(cluster_tfidf) > 0:
                # ã‚¯ãƒ©ã‚¹ã‚¿å†…ã§ã®TF-IDFå¹³å‡å€¤ã‚’è¨ˆç®—
                avg_tfidf = cluster_tfidf.mean()
                top_indices = avg_tfidf.nlargest(20).index
                top_words = []
                top_scores = []
                
                for idx in top_indices:
                    tfidf_idx = int(idx.split('_')[1])  # tfidf_0 -> 0
                    if tfidf_idx < len(vocab):
                        top_words.append(vocab[tfidf_idx])
                        top_scores.append(avg_tfidf[idx])
                
                if top_words:
                    with st.expander(f"Cluster {i} ãƒˆãƒƒãƒ—èªžå½™", expanded=False):
                        word_df = pd.DataFrame({
                            'èªžå½™': top_words,
                            'TF-IDFå¹³å‡': top_scores
                        })
                        
                        # æ£’ã‚°ãƒ©ãƒ•
                        fig_words = px.bar(
                            word_df.head(10),
                            x='TF-IDFå¹³å‡',
                            y='èªžå½™',
                            orientation='h',
                            title=f'Cluster {i} ãƒˆãƒƒãƒ—10èªžå½™'
                        )
                        fig_words.update_layout(height=400)
                        st.plotly_chart(fig_words, use_container_width=True)
                        
                        # ãƒ†ãƒ¼ãƒ–ãƒ«
                        st.dataframe(word_df, hide_index=True)

    st.success("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æžãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
else:
    st.info("ðŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã‚’è¡Œã„ã€ã€Œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")